"use client";

import { useState, useCallback, useRef, useEffect } from 'react';
import { speechToText } from '@/services/asr';
import { chatStream } from '@/services/chat';
import { streamTextToSpeech } from '@/services/tts';
import { useChatStore } from '@/store/chatStore';
import { useAvatarStore } from '@/store/avatarStore';
import { useLanguageStore } from '@/store/languageStore';

/** ç”Ÿæˆå”¯ä¸€ ID */
function generateId(): string {
  return `${Date.now()}-${Math.random().toString(36).slice(2, 9)}`;
}

/**
 * è¯­éŸ³äº¤äº’ Hook
 * å¤„ç†å®Œæ•´çš„è¯­éŸ³äº¤äº’æµç¨‹ï¼šASR -> Chat -> TTS -> æ’­æ”¾
 */
export function useVoiceInteraction() {
  const [isProcessing, setIsProcessing] = useState(false);
  const { addMessage, updateMessageContent, updateMessageStatus } = useChatStore();
  const { setAction } = useAvatarStore();
  const { language } = useLanguageStore();
  
  const audioQueueRef = useRef<Array<{ audio: HTMLAudioElement; url: string }>>([]);
  const playingRef = useRef(false);
  const drainResolvers = useRef<Array<() => void>>([]);

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†éŸ³é¢‘èµ„æº
  useEffect(() => {
    return () => {
      // æ¸…ç†é˜Ÿåˆ—
      for (const item of audioQueueRef.current) {
        item.audio.pause();
        URL.revokeObjectURL(item.url);
      }
      audioQueueRef.current = [];
      playingRef.current = false;
    };
  }, []);

  const resolveDrain = useCallback(() => {
    if (audioQueueRef.current.length === 0 && !playingRef.current) {
      drainResolvers.current.forEach((fn) => fn());
      drainResolvers.current = [];
    }
  }, []);

  const playNextRef = useRef<() => void>(() => {});
  useEffect(() => {
    playNextRef.current = () => {
      if (playingRef.current) return;
      const next = audioQueueRef.current.shift();
      if (!next) {
        setAction('idle');
        resolveDrain();
        return;
      }

      playingRef.current = true;
      const { audio, url } = next;

      audio.onplay = () => setAction('talk');
      audio.onended = () => {
        URL.revokeObjectURL(url);
        playingRef.current = false;
        playNextRef.current();
      };
      audio.onerror = (e) => {
        console.error('éŸ³é¢‘æ’­æ”¾é”™è¯¯:', e);
        URL.revokeObjectURL(url);
        playingRef.current = false;
        playNextRef.current();
      };

      audio.play().catch((err) => {
        console.error('éŸ³é¢‘æ’­æ”¾å¤±è´¥:', err);
        URL.revokeObjectURL(url);
        playingRef.current = false;
        playNextRef.current();
      });
    };
  }, [resolveDrain, setAction]);

  const enqueueAudio = useCallback(
    
    
    (bytes: Uint8Array) => {
      // Copy into a fresh Uint8Array to avoid SharedArrayBuffer typing issues
      const safeBytes = new Uint8Array(bytes);
      const blob = new Blob([safeBytes], { type: 'audio/mp3' });
      const url = URL.createObjectURL(blob);
      const audio = new Audio(url);
      audioQueueRef.current.push({ audio, url });
      playNextRef.current();
    },
    []
  );

  const waitForDrain = useCallback(() => {
    if (!playingRef.current && audioQueueRef.current.length === 0) return Promise.resolve();
    return new Promise<void>((resolve) => {
      drainResolvers.current.push(resolve);
    });
  }, []);

  // å¤„ç†æ–‡æœ¬è¾“å…¥ï¼ˆæµå¼è¯­éŸ³è¯†åˆ«åç›´æ¥è°ƒç”¨ï¼‰
  const handleTextInput = useCallback(async (userText: string) => {
    if (isProcessing || !userText.trim()) return;
    
    setIsProcessing(true);
    const msgId = generateId();
    const botMsgId = generateId();

    try {
      console.log('ğŸ“ å¤„ç†ç”¨æˆ·è¾“å…¥:', userText);

      // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
      addMessage({
        id: msgId,
        role: 'user',
        content: userText,
        status: 'success',
        timestamp: Date.now(),
      });

      // æ·»åŠ  AI å ä½æ¶ˆæ¯
      addMessage({
        id: botMsgId,
        role: 'ai',
        content: '',
        status: 'loading',
        timestamp: Date.now(),
      });

      // Chat: å‘é€ç»™ AIï¼ˆæµå¼å“åº”ï¼‰
      let fullBotResponse = '';
      console.log('ğŸ¤– å‘é€ç»™ AI...');
      
      await chatStream(
        userText,
        (chunk) => {
          fullBotResponse += chunk;
          updateMessageContent(botMsgId, fullBotResponse);
        },
        {
          language,
          systemPrompt: 'Please respond in English.',
        }
      );
      
      updateMessageStatus(botMsgId, 'success');
      console.log('ğŸ¤– AI å›å¤:', fullBotResponse);
      
      if (!fullBotResponse) {
        throw new Error('AI å›å¤ä¸ºç©º');
      }

      console.log('ğŸ”Š æµå¼ç”Ÿæˆè¯­éŸ³...');
      await streamTextToSpeech(fullBotResponse, {
        onAudio: async (bytes) => {
          enqueueAudio(bytes);
        },
      });

      await waitForDrain();

    } catch (error) {
      console.error('è¯­éŸ³äº¤äº’é”™è¯¯:', error);
      setAction('idle');
      updateMessageStatus(botMsgId, 'error');
    } finally {
      setIsProcessing(false);
    }
  }, [
    addMessage,
    updateMessageContent,
    updateMessageStatus,
    setAction,
    isProcessing,
    enqueueAudio,
    waitForDrain,
    language,
  ]);

  // å¤„ç†è¯­éŸ³è¾“å…¥ï¼ˆå½•éŸ³åè°ƒç”¨ï¼Œéœ€è¦å…ˆ ASRï¼‰
  const handleVoiceInput = useCallback(async (audioBlob: Blob) => {
    if (isProcessing) return;
    
    setIsProcessing(true);
    const botMsgId = generateId();

    try {
      // ASR: è¯­éŸ³è½¬æ–‡å­—
      console.log('ğŸ¤ è¯­éŸ³è¯†åˆ«ä¸­...');
      const userText = await speechToText(audioBlob);
      console.log('ğŸ¤ è¯†åˆ«ç»“æœ:', userText);

      if (!userText.trim()) {
        setIsProcessing(false);
        return;
      }

      // é‡ç½®å¤„ç†çŠ¶æ€ï¼Œè®© handleTextInput æ¥ç®¡
      setIsProcessing(false);
      await handleTextInput(userText);

    } catch (error) {
      console.error('è¯­éŸ³è¯†åˆ«é”™è¯¯:', error);
      setAction('idle');
      setIsProcessing(false);
      updateMessageStatus(botMsgId, 'error');
    }
  }, [handleTextInput, setAction, updateMessageStatus, isProcessing]);

  return {
    isProcessing,
    handleVoiceInput,
    handleTextInput,
  };
}
